<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The State-of-the-Art and Challenges of Data Stream Clustering Algorithms in Practice | Online clustering: algorithms, evaluation, metrics, application and benchmarking using River</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="The State-of-the-Art and Challenges of Data Stream Clustering Algorithms in Practice" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Tutorial presented at the 32nd International Joint Conference on Artificial Intelligence, 19th - 25th August 2023, Macau, S.A.R." />
<meta property="og:description" content="Tutorial presented at the 32nd International Joint Conference on Artificial Intelligence, 19th - 25th August 2023, Macau, S.A.R." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Online clustering: algorithms, evaluation, metrics, application and benchmarking using River" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The State-of-the-Art and Challenges of Data Stream Clustering Algorithms in Practice" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Tutorial presented at the 32nd International Joint Conference on Artificial Intelligence, 19th - 25th August 2023, Macau, S.A.R.","headline":"The State-of-the-Art and Challenges of Data Stream Clustering Algorithms in Practice","name":"Online clustering: algorithms, evaluation, metrics, application and benchmarking using River","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=2e84e4e6a7702028d05f8e644818c6eaed1e8047">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'true', 'auto');
    ga('send', 'pageview');
  </script>



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->


  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 


  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">The State-of-the-Art and Challenges of Data Stream Clustering Algorithms in Practice</h1>
      <h2 class="project-tagline">Tutorial presented at the 32<sup>nd</sup> International Joint Conference on Artificial Intelligence, 19<sup>th</sup> - 25<sup>th</sup> August 2023, Macau, S.A.R.</h2>
      
        <a href="./index.html" class="btn">Homepage</a>
        <a href="./related-materials.html" class="btn">Related materials</a>
      
      
        <a href="https://online-ml.github.io/deep-river/" class="btn">deep-river's webpage</a>
        <a href="https://2023.ecmlpkdd.org/" class="btn">ECML-PKDD 2023</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <style type="text/css">
  .image-left {
    display: block;
    margin-left: auto;
    margin-right: auto;
    float: right;
  }
</style>

<h1 id="practical-information">Practical information</h1>

<p>The agenda of the tutorial will be as follows:</p>

<ul>
  <li><strong>Place</strong>: Polytechnico di Torino</li>
  <li><strong>Time</strong>: Friday, September 22<sup>nd</sup> 2023, afternoon (GMT+1)</li>
</ul>

<h1 id="abstract">Abstract</h1>

<p>With significant advantages upon run time, resource usage, and complexity, online machine learning and stream clustering algorithms are playing a critical role in data science. Besides substantial resource advantages, these algorithms achieve a comparable performance to traditional batch machine learning methods. This tutorial will first surveys online machine learning and data stream clustering. With the emergence of fairness and interpretability, we will also discuss certain attempts in inducing a fair and interpretable machine learning model for data streams. We will then put the tutorial into a practical context with <code class="language-plaintext highlighter-rouge">River</code>, a Python library resulted from a merge between <code class="language-plaintext highlighter-rouge">Creme</code> and <code class="language-plaintext highlighter-rouge">scikit-multiflow</code>. We will also illustrate <code class="language-plaintext highlighter-rouge">River</code> as a go-to platform for online machine learning development by providing a guidance on how to integrate an algorithm with a clear workflow, alongside actual examples of past problems and solutions during the development process.</p>

<p>Besides, during this tutorial, state-of-the-art algorithms, associated core research approaches, and future directions of data stream clustering will be presented. Truly incremental clustering validity indices will also be mentioned as an important part of the stream clustering process and will be investigated thoroughly. Currently available metrics require the information of all past points, which is impractical for unlimited data streams. <code class="language-plaintext highlighter-rouge">River</code> is the first package to design and deploy such incremental indices.</p>

<p>Last but not least, the tutorial will demonstrate the use of <code class="language-plaintext highlighter-rouge">River</code> and the associated clustering module in real-world scenarios. From this, we propose methods of clustering configuration, hyper-parameter tuning, applications and settings for benchmarking using real-world problems and datasets. Preliminary benchmarking results will also be provided to showcase the advantages and consistency in the performance of implemented algorithms.</p>

<h1 id="two-sentence-tutorial-description">Two-sentence tutorial description</h1>

<p><em>(to be included into the conference registration brochure)</em></p>

<p>This tutorial provides an in-depth survey to online (data stream) machine learning, with an emphasis on fairness and interpretability, which is then later put into a practical context with \texttt{River}, a go-to Python library for the task. Moreover, data stream clustering problems will also be further rigorously investigated by mentioning state-of-the-art-algorithms, solutions for the implementation of incremental clustering validity indices, associated core research approaches, and future directions.</p>

<h1 id="two-paragraph-tutorial-description">Two-paragraph tutorial description</h1>

<p><em>(suitable for a web page overview, as per requested by the Call for Tutorials from IJCAI 23)</em></p>

<p>With significant advantages upon run time, resource usage, and complexity, online machine learning and stream clustering algorithms are playing a critical role in data science. Besides substantial resource advantages, these algorithms achieve a comparable performance to traditional batch machine learning methods. As such, the first part of this tutorial is devoted as a literature survey into the field, including a strong emphasis on fairness and interpretability, which is then put into a practical context with <code class="language-plaintext highlighter-rouge">River</code>, a go-to Python library resulted from a merge between <code class="language-plaintext highlighter-rouge">Creme</code> and <code class="language-plaintext highlighter-rouge">scikit-multiflow</code>.</p>

<p>Besides, we will also investigate the problem of data stream clustering rigorously with state-of-the-art algorithms, associated core research approaches and future directions. In parallel, we will also look into the design of incremental clustering validity indices, which is an important part of the benchmarking process. All preliminary results will be provided at the end to showcase the advantages and consistency in the performance of implemented algorithms within <code class="language-plaintext highlighter-rouge">River</code>.</p>

<h1 id="motivation">Motivation</h1>

<p>Having algorithms at hand that can process data that arrives continuously in the form of data streams is crucial. 
Online Learning potentially has to deal with real-time data rather than previously known data sets. 
To deal with the evaluation and application of models on data streams, Bifet et al. [5] defined the online learning requirements as follows:</p>
<ul>
  <li>Process an instance at a time, and inspect it (at most) once.</li>
  <li>Use a limited amount of time to process each instance.</li>
  <li>Use a limited amount of memory.</li>
  <li>Be ready to give an answer (e.g. prediction) at any time</li>
  <li>Adapt to temporal changes.</li>
</ul>

<p>The following figure depicts how an online learning framework is able to comply with the data stream requirements for supervised learning tasks. 
The model processes labeled data points $\left(\overrightarrow{x},y\right)$ by updating the model while instead predicting a label $\hat{y}$ for each unlabeled instance $\overrightarrow{x}$. 
Thus, the model processes each instance from an evolving data stream, updates the underlying model, and is ready to predict at any time.</p>

<p>Even until now, the development of stream algorithms is quite scattered and decentralized. Previously, algorithms were usually self-developed and maintained by the respective authors in various different programming languages, with none of the existing frameworks being widely adopted within the online learning community. Currently, <code class="language-plaintext highlighter-rouge">River</code> is becoming not only a go-to library for online machine learning tasks, but also a pioneer framework for the implementation of any new algorithm within the field.</p>

<figure>
  <img src="./stream-structure.png" alt="stream-structure" />
  <figcaption><strong>Figure 1</strong>: Structure of the interaction between data stream and prediction model.</figcaption>
</figure>

<p>A significant question in the context of the advancement of <code class="language-plaintext highlighter-rouge">River</code> is whether deep learning algorithms, which have been a staple in many batch learning frameworks for some time, can also fulfill the requirements and therefore be applied within online learning environments. To this end, we developed <code class="language-plaintext highlighter-rouge">deep-river</code> which combines the <code class="language-plaintext highlighter-rouge">River</code> API for online learning algorithms and <code class="language-plaintext highlighter-rouge">PyTorch</code> for the flexible development of neural architectures.
Based on <code class="language-plaintext highlighter-rouge">River</code> and the newly developed framework <code class="language-plaintext highlighter-rouge">deep-river</code>, we present in this tutorial the chances and pitfalls for online deep learning by</p>
<ul>
  <li>addressing the online learning requirements,</li>
  <li>the <code class="language-plaintext highlighter-rouge">River</code> API and discussing their</li>
  <li>applicability in deep learning architectures.</li>
</ul>

<p>The tutorial will cover the transition from simple conventional machine learning models to sophisticated neural architectures while considering not only classification, regression and anomaly detection metrics, but also time and memory consumption which are key factors for the throughput of the underlying model.</p>

<h1 id="presenters-bibliography">Presenters’ bibliography</h1>

<p>The following authors will be in-person presenters, i.e., tutors who will attend IJCAI 2023 and present part of the tutorial: <strong>Jacob Montiel</strong>, <strong>Hoang-Anh Ngo</strong>, <strong>Minh-Huong Le-Nguyen</strong> and <strong>Albert Bifet</strong>.</p>

<p><img src="presenter-pics/jacob-montiel.jpg" alt="drawing" width="220" style="border-radius:60%" class="image-left" /></p>

<p><strong>Jacob Montiel</strong> is currently a Data Scientist in AWS Security. He is formerly a research fellow at the University of Waikato, New Zealand and Adjunct Researcher in the DIG Team at Télécom Paris, Institut Polytechnique de Paris, France and the core developer and maintainer of <code class="language-plaintext highlighter-rouge">River</code>.</p>

<p>His research interests are in the field of machine learning for evolving data streams. Prior to focusing on research, Jacob led the development work for onboard software for aircraft and engine’s prognostics at GE Aviation; working in the development of GE’s Brilliant Machines, part of the IoT and GE’s approach to Industrial Big Data.</p>

<p><br clear="left" /></p>

<p><img src="presenter-pics/hoang-anh.ngo.jpg" alt="drawing" width="220" style="border-radius:60%" class="image-left" /></p>

<p><strong>Hoang-Anh Ngo</strong> is currently supported by the AI Institute and the School of Computing and Mathematical Sciences, University of Waikato under an External Study Award (ESA) to support his research on <code class="language-plaintext highlighter-rouge">River</code>, the machine learning library in Python for data streams.</p>

<p>His research interests lies in the field of machine learning for evolving data stream, particularly in online clustering and classification algorithms. Previously, he joined the team of IT Specialists in COVID-19 task force, formed by the Ministry of Health of Vietnam as a Epidemiological Modelling Unit head.</p>

<p><br clear="left" /></p>

<p><img src="presenter-pics/minh-huong.le-nguyen.jpg" alt="drawing" width="220" style="border-radius:60%" class="image-left" /></p>

<p><strong>Minh-Huong Le-Nguyen</strong> is a third-year doctoral student at LCTI, Télécom Paris, Institut Polytechnique de Paris in France. Her doctoral research focuses on the applications of machine learning on data streams to implement predictive maintenance in the railway industry. She received her Bachelor’s degree in Computer Science at University Pierre and Marie Curie (France) in 2013, then she graduated from Télécom Paris with a Master’s degree in Data Science in 2019.</p>

<p><br clear="left" /></p>

<p><img src="presenter-pics/albert-bifet.jpg" alt="drawing" width="220" style="border-radius:60%" class="image-left" /></p>

<p><strong>Albert Bifet</strong> is a Professor of AI and the DIrector of the Te Ipu o te Mahara AI Institute  at University of Waikato, and Professor of Big Data at Data, Intelligence and Graphs (DIG) LTCI, Télécom Paris. Problems he investigate are motivated by large scale data, the Internet of Things (IoT), and Big Data Science. He co-leads the open source projects MOA (Massive On-line Analysis), Apache SAMOA (Scalable Advanced Massive Online Analysis) and StreamDM.</p>

<p>Website: <a href="https://albertbifet.com/">https://albertbifet.com/</a></p>

<h1 id="presenters-contact-information">Presenters’ contact information</h1>

<h3 id="cedric-kulbach">Cedric Kulbach</h3>

<p>  FZI Research Center for Information Technology, Karlsruhe, Germany</p>

<p>  Email: <a href="mailto:kulbach@fzi.de">kulbach@fzi.de</a></p>

<h3 id="lucas-cazzonelli">Lucas Cazzonelli</h3>

<p>  FZI Research Center for Information Technology, Karlsruhe, Germany</p>

<p>  Email: <a href="mailto:cazzonelli@fzi.de">cazzonelli@fzi.de</a></p>

<h3 id="hoang-anh-ngo">Hoang-Anh Ngo</h3>

<p>  Artificial Intelligence Institute, University of Waikato, Hamilton, New Zealand</p>

<p>  Email: <a href="mailto:h.a.ngo@sms.ed.ac.uk">h.a.ngo@sms.ed.ac.uk</a></p>

<h3 id="minh-huong-le-nguyen">Minh-Huong Le Nguyen</h3>

<p>  LCTI, Télécom Paris, Institut Polytechnique de Paris, France</p>

<p>  Email: <a href="mailto:minh.lenguyen@telecom-paris.fr">minh.lenguyen@telecom-paris.fr</a></p>

<h3 id="albert-bifet">Albert Bifet</h3>

<p>  Artificial Intelligence Institute, University of Waikato, Hamilton, New Zealand and LCTI, Télécom Paris, Institut Polytechnique de Paris, France</p>

<p>  Email: <a href="mailto:abifet@waikato.ac.nz">abifet@waikato.ac.nz</a></p>

<h1 id="intended-audience">Intended audience</h1>

<p>The target audience of the tutorial includes any researchers and practitioners with interests in machine learning for big data, evolving data streams or IoT applications.</p>

<p>Basic knowledge with the Python programming language would be necessary. Apart from that, there will be no particular requirements or prerequisites on previous experience on data stream learning. However, either experience with traditional machine learning frameworks (<code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">keras</code>, <code class="language-plaintext highlighter-rouge">pytorch</code>, etc.) or previous interactions with online machine learning packages/tools, for example <code class="language-plaintext highlighter-rouge">MOA</code> (in Java), <code class="language-plaintext highlighter-rouge">stream</code> in <code class="language-plaintext highlighter-rouge">R</code>, <code class="language-plaintext highlighter-rouge">scikit-multiflow</code>, <code class="language-plaintext highlighter-rouge">Creme</code> or <code class="language-plaintext highlighter-rouge">River</code> in Python, would be beneficial.</p>

<table>
  <tbody>
    <tr>
      <td>For any developer who wants to contribute to <code class="language-plaintext highlighter-rouge">River</code> or use <code class="language-plaintext highlighter-rouge">River</code> to employ their own research work, a thorough understanding of <code class="language-plaintext highlighter-rouge">Git</code>, functionalities of <code class="language-plaintext highlighter-rouge">GitHub</code> (how to open a pull request, an issue, a discussion, Github Actions, etc.), code formatters in Python (<code class="language-plaintext highlighter-rouge">flake8</code>, <code class="language-plaintext highlighter-rouge">black</code></td>
      <td>, <code class="language-plaintext highlighter-rouge">isort</code>, etc.) would be necessary.</td>
    </tr>
  </tbody>
</table>

<h1 id="format-and-detailed-schedule">Format and detailed schedule</h1>

<p>The tutorial is intended to be of <strong>3.5 hours</strong> (half-day, consisting of <strong>two 1:45h slots</strong>), spread throughout <strong>3 sections</strong> with <strong>two 15-minute breaks</strong> between each section. The detailed outline of the tutorial is as follows:</p>

<ol>
  <li>Introduction to data stream (online) machine learning (<strong>1 hour</strong>):
    <ol>
      <li>What is online machine learning, and why do we need online machine learning?</li>
      <li>Differences, advantages and disadvantages of online machine learning compared to batch/traditional machine learning.</li>
      <li>Methods and interventions to induce fairness and interpretability in machine learning for streaming data in general.</li>
      <li>Introduction to <code class="language-plaintext highlighter-rouge">River</code>:
        <ol>
          <li>Its foundation as a merge between <code class="language-plaintext highlighter-rouge">Creme</code> and <code class="language-plaintext highlighter-rouge">scikit-multiflow</code>;</li>
          <li>Design principles;</li>
          <li>Major advantages of <code class="language-plaintext highlighter-rouge">River</code> towards its competitors;</li>
          <li>Major updates/improvements throughout the versions.</li>
        </ol>
      </li>
      <li>A brief guidance on how to develop/implement a model within <code class="language-plaintext highlighter-rouge">River</code>, along with demo and examples of past problems and solutions within the development process.</li>
      <li>Future maintenance and development orientation for <code class="language-plaintext highlighter-rouge">River</code>.</li>
    </ol>
  </li>
  <li>Online clustering algorithms and evaluation metrics (<code class="language-plaintext highlighter-rouge">1 hour 15 minutes</code>):
    <ol>
      <li>A literature survey on existing clustering algorithms, the general concepts, approaches and their evolution.</li>
      <li>Introduction to the state-of-the-art clustering algorithms implemented in <code class="language-plaintext highlighter-rouge">River</code> and their potential differences or advantages compared to previously implemented versions.</li>
      <li>How to improve accuracy in calculating micro-cluster centers and diameters through time using Welford’s algorithm.</li>
      <li>An investigation into currently available static validity indices, arising problems and motivation for the foundation of their truly incremental versions.</li>
      <li>A comparative survey on the expansion of incremental clustering validity indices, particularly among the most significant and widely used one that are adapted and integrated to <code class="language-plaintext highlighter-rouge">River</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">textClust</code>, the first text clustering algorithm to be implemented in <code class="language-plaintext highlighter-rouge">River</code>.</li>
      <li>A brief introduction to research on fairness and interpretability of data stream clustering algorithms.</li>
      <li>Potential future research directions towards improvement in time and accuracy of stream clustering.</li>
    </ol>
  </li>
  <li>Use cases and benchmarking (<strong>45 minutes</strong>):
    <ol>
      <li>Practical applications.</li>
      <li>Comparison between online and traditional/batch clustering algorithms.</li>
      <li>Live visualization of stream algorithms and their results in synthetic and real-life scenarios.</li>
      <li>Motivation, setting and system requirements for conducting benchmarking.</li>
      <li>Tutorial on benchmarking using the River package and the associated available <code class="language-plaintext highlighter-rouge">git</code> repository and terminal.</li>
      <li>Preliminary benchmarking results.</li>
    </ol>
  </li>
</ol>

<h1 id="brief-outline">Brief outline</h1>

<h2 id="introduction-to-data-stream-machine-learning">Introduction to data stream machine learning</h2>

<p>This first part is intended to provide the motivation and necessity of online stream learning. As a matter of fact, traditional machine learning methods can not deal with an particularly large amount of data with limited resources and time constrains, which means that there is an urgent need for specific data stream machine learning methods with comparable results.</p>

<p>Besides providing insights on advantages and disadvantages of online machine learning, we will also provide an introduction to <code class="language-plaintext highlighter-rouge">River</code>, a Python library aimed to become a go-to toolkit for this purpose with numerous advantages and features towards its open source competitors. Not only will we present <code class="language-plaintext highlighter-rouge">River</code> as a tool, we will also provide a detailed guide on how to contribute to <code class="language-plaintext highlighter-rouge">River</code> or utilize <code class="language-plaintext highlighter-rouge">River</code> to facilitate participants’ own research works.</p>

<p>To conclude this section,  we present the latest trends in research for fairness and interpretability of stream machine learning models. Having to handle an unlimited amount of data while having to maintain the accuracy under concept drifts, the research for a fair and interpretable AI is interesting, yet much more demanding compared to that of traditional machine learning models.</p>

<h2 id="a-literature-survey-on-online-clustering-algorithms-and-metrics">A literature survey on online clustering algorithms and metrics</h2>

<p>This part will first start with an extensive survey on online clustering algorithms. First, we will start with the development from the first algorithms that introduced the concept of micro-clusters/macro-clusters and online/offline phases (BIRCH/CluStream), then to the evolution based on different approaches. These approaches include either distance-based, grid-based, model-based or projected, two-phase, type of time windows (damped, sliding, landmark or pyramidal), or the use of medoids/centroids. Besides interpreting these approaches, respective algorithms and their implementations within <code class="language-plaintext highlighter-rouge">River</code> are also introduced, including KMeans, DenStream, DBStream, STREAMKMeans and EvoStream.</p>

<p>With the emerging research on fairness and interpretability of AI, we will also discuss certain attempts in inducing a fair and interpretable stream clustering algorithm, including</p>

<ul>
  <li>The first attempts for a fair K-Means algorithm by Schmidt et al. (2018) \cite{DBLP:journals/corr/abs-1812-10854} or fair k-Center algorithm by Bera et al. (2022) \cite{10.1145/3485447.3512188} have been introduced.</li>
  <li>Intepretable multiple data stream clustering with clipped stream representation has also been proposed by Laurinec and Lucká (2019) \cite{10.1007/s10618-018-0598-2}.</li>
</ul>

<p>From this, we will discuss their effectiveness and later on how to apply these certain approaches for a broader family of clustering algorithms.</p>

<p>Finally, one aspect of online clustering algorithms that are usually neglected is the use of incremental validation metrics. Currently, apart from <code class="language-plaintext highlighter-rouge">River</code>, there is no tool/package that facilitates the use of truly incremental metrics, i.e. metrics that only use the summary statistics and the latest observation instead of having to use information of all passed points, which is impractical in stream learning. As such, in this part, we will also focus on the construction and comparison between these metrics, and also how to apply them in analyzing clustering algorithms’ performances when put into practice.</p>

<h2 id="practical-applications-and-benchmarking-using-the-clustering-module-of-river">Practical applications and benchmarking using the clustering module of <code class="language-plaintext highlighter-rouge">River</code></h2>

<p>The final part serves as a practical demonstration on the usage of <code class="language-plaintext highlighter-rouge">River</code> and the associated clustering module in real-life scenarios.</p>

<p>First, a brief demonstration of <code class="language-plaintext highlighter-rouge">River</code> will be presented and its essential functionalities will also be compared with respective traditional/batch machine learning algorithms in terms of performance, memory and time usage to prove that although online methods takes up less resources, they have the ability to obtain a similar accuracy.</p>

<p>The next part will be dedicated to stream clustering algorithm’s benchmarking work. The setting, system requirement, benchmarking method and hyper-parameter tuning will all be discussed.</p>

<p>Last but not least, preliminary benchmarking results with dedicated datasets will also be provided to exhibit the advantages and consistency in the performance of implemented algorithms.</p>

<h1 id="type-of-support-material-to-be-supplied-to-attendees">Type of support material to be supplied to attendees</h1>

<p>Participants will receive the following support material:</p>

<ul>
  <li>Presentation slides;</li>
  <li>PDF versions (including results) of any quizzes and surveys within the tutorial;</li>
  <li>All Jupyter notebooks and demos executed within and/or related to the tutorial.</li>
</ul>

<p>Apart from the printed materials, the electronic version will all be freely, publicly available on the dedicated tutorial’s website.</p>

<h1 id="list-of-previous-offerings-of-the-tutorial-and-relationship-with-them">List of previous offerings of the tutorial and relationship with them</h1>

<h2 id="list-of-previous-offerings-of-the-tutorial">List of previous offerings of the tutorial</h2>

<p>There has been two previous offerings of similar tutorials on the topic at highly-ranked conferences, including:</p>

<ul>
  <li>First offering:
    <ul>
      <li><strong>Title:</strong> Online Clustering: Algorithms, Evaluation, Metrics, Applications and Benchmarking using <code class="language-plaintext highlighter-rouge">River</code>.</li>
      <li><strong>Conference:</strong> The 26th Pacific - Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2022).</li>
      <li><strong>Number of participants:</strong>} Unknown. Due to the COVID-19 prevention measures and time differences, the tutorial is presented online with a pre-recorded video.</li>
    </ul>
  </li>
  <li>Second offering:
    <ul>
      <li><strong>Title:</strong> Online Clustering: Online Clustering: Algorithms, Evaluation, Metrics, Applications and Benchmarking \cite{10.1145/3534678.3542600}.</li>
      <li><strong>Conference:</strong> The 28th ACM SIGKDD Cofnerence on Knowledge Discovery and Data Mining (KDD ‘22).</li>
      <li><strong>Content:</strong> Publicly available within \href<a href="https://hoanganhngo610.github.io/river-clustering.kdd.2022/">the tutorial’s website</a> and <a href="https://dl.acm.org/doi/10.1145/3534678.3542600">the conference’s proceedings</a>.</li>
      <li><strong>Number of participants:</strong> Approximately 50 participants.</li>
    </ul>
  </li>
</ul>

<h2 id="relationship-to-previous-editions">Relationship to previous editions</h2>

<h3 id="similarities">Similarities</h3>

<p>This tutorial will preserve its strong points from previous editions by</p>
<ul>
  <li>providing a deep, thorough literature survey of state-of-the-art stream clustering algorithms and future research directions; and</li>
  <li>allowing participants to have essential understanding of online learning, put under a practical point of view by <code class="language-plaintext highlighter-rouge">River</code>. Moreover, they will also be provided with interactive  demonstration that simulates real-life scenarios.</li>
</ul>

<h3 id="differences">Differences</h3>

<p>The main differences of this tutorial compared to its previous versions are:</p>

<ul>
  <li>First and foremost, this tutorial will be more developer-oriented. This means that, apart from providing guidance and use cases <code class="language-plaintext highlighter-rouge">River</code>, the authors also provide a detailed picture of the developing process and how to contribute/develop a user’s own algorithm within the ecosystem, along with actual problems and solutions during the development process. This is expected to potentially solve the problem of scattered and unorganized implementation of currently available stream clustering algorithms.</li>
  <li>On top of that, a clear development and maintenance orientation under the form of a public roadmap will also be provided. This can be considered as one of the most crucial parts in the development of any open-source projects.</li>
  <li>The tutorial will also tackle the problems and provide detailed insights as a literature review of fairness and interpretability of online machine learning algorithms in general and stream clustering algorithms in particular. This part is intended to address a comment on the tutorial offering at KDD’22. Moreover, this will also be the first tutorial to ever conduct a survey into this aspect of online machine learning.</li>
  <li>This tutorial will also be the first to analyze the implementation strategy of truly incremental clustering metrics in <code class="language-plaintext highlighter-rouge">River</code>, which are not available in any other online machine learning tools/packages.</li>
  <li>Apart from providing a guidance on how to conduct benchmarking with <code class="language-plaintext highlighter-rouge">River</code>, preliminary results for comparison will also be provided. This work has just been completed recently and were not provided within any previous editions of this tutorial.</li>
  <li>The tutorial will also introduce a starting solution to new research pathways in stream clustering algorithms:
    <ul>
      <li>Text clustering algorithms (despite the fact that previously, text clustering can be done with an indirect approach, using Term Frequency - Inverse Document Frequency (TF-IDF) \cite{rajaraman_ullman_2011} with an arbitrary numerical stream clustering algorithm;</li>
      <li>Improvement in accuracy of calculating micro-clusters’ centers and diameters using Welford’s algorithm \cite{10.1007/978-3-642-51461-6_3} instead of cluster feature vectors.</li>
    </ul>
  </li>
</ul>

<h1 id="related-materials">Related materials</h1>

<p>For all related materials, including presentation slides, demos, source code, related papers and any other piece of information, please visit <a href="./related-materials.html">this page</a>.</p>

<h1 id="citation">Citation</h1>

<p>TBA</p>

<h1 id="references">References</h1>

<ol>
  <li>Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep Learning with Differential Privacy. In <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, CCS ’16, pages 308–318, New York, NY, USA, October 2016. Association for Computing Machinery.</li>
  <li>Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online Continual Learning with Maximally Interfered Retrieval, October 2019.</li>
  <li>Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning, October 2019.</li>
  <li>Atilim Gunes Baydin, Robert Cornish, David Martínez-Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hyper-gradient descent. In <em>6th international conference on learning representations, ICLR 2018, vancouver, BC, canada, april 30 - may 3, 2018, conference track proceedings</em>. OpenReview.net, 2018. tex.bibsource: dblp computer science bibliography, <a href="https://dblp.org">https://dblp.org</a> tex.biburl: <a href="https://dblp.org/rec/conf/iclr/BaydinCMSW18.bib">https://dblp.org/rec/conf/iclr/BaydinCMSW18.bib</a> tex.timestamp: Thu, 23 Apr 2020 11:53:22 +0200.</li>
  <li>Albert Bifet, Ricard Gavaldà, Geoff Holmes, and Bernhard Pfahringer. <em>Machine Learning for Data Streams with Practical Examples in MOA</em>. MIT Press, 2018.</li>
  <li>Nadia Burkart and Marco F. Huber. A Survey on the Explainability of Supervised Machine Learning. <em>Journal of Artificial Intelligence Research</em>, 70:245–317, January 2021.</li>
  <li>Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark Experience for General Continual Learning: A Strong, Simple Baseline, October 2020.</li>
  <li>Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. In <em>2017 IEEE Symposium on Security and Privacy (SP)</em>, pages 39–57, May 2017.</li>
  <li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.</li>
  <li>João Gama, Pedro Medas, Gladys Castillo, and Pedro Rodrigues. Learning with drift detection. In Ana L. C. Bazzan and Sofiane Labidi, editors, <em>Advances in Artificial Intelligence – SBIA 2004</em>, pages 286–295, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg.</li>
  <li>Heitor Murilo Gomes, Jesse Read, and Albert Bifet. Streaming random patches for evolving data stream classification. In <em>2019 IEEE International Conference on Data Mining (ICDM)</em>, pages 240–249, 2019.</li>
  <li>Heitor Murilo Gomes, Jesse Read, Albert Bifet, Jean Paul Barddal, and João Gama. Machine learning for streaming data: State of the art, challenges, and opportunities. <em>SIGKDD Explor. Newsl.</em>, 21(2):6–22, nov 2019.</li>
  <li>Max Halford, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, and Adil Zouitine. creme, a Python library for online machine learning, 2019.</li>
  <li>Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked Autoencoders Are Scalable Vision Learners. In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 15979–15988, New Orleans, LA, USA, June 2022. IEEE.</li>
  <li>Steven C. H. Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online Learning: A Comprehensive Survey, October 2018.</li>
  <li>Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. <em>Neural Networks</em>, 2(5):359–366, January 1989.</li>
  <li>Lakhmi C. Jain, Manjeevan Seera, Chee Peng Lim, and P. Balasubramaniam. A review of online learning in supervised neural networks. <em>Neural Computing and Applications</em>, 25(3-4):491–509, September 2014.</li>
  <li>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. <em>Proceedings of the National Academy of Sciences</em>, 114(13):3521–3526, March 2017.</li>
  <li>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In <em>Advances in Neural Information Processing Systems</em>, volume 25. Curran Associates, Inc., 2012.</li>
  <li>Zhizhong Li and Derek Hoiem. Learning without Forgetting. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 40(12):2935–2947, December 2018.</li>
  <li>Jesus L Lobo, Javier Del Ser, Albert Bifet, and Nikola Kasabov. Spiking neural networks and online learning: An overview and perspectives. <em>Neural Networks</em>, 121:88–100, 2020. Publisher: Elsevier.</li>
  <li>Scott Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions, November 2017.</li>
  <li>Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks, September 2019.</li>
  <li>Michael McCloskey and Neal J. Cohen. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. In <em>Psychology of Learning and Motivation</em>, volume 24, pages 109–165. Elsevier, 1989.</li>
  <li>Jacob Montiel, Jesse Read, Albert Bifet, and Talel Abdessalem. Scikit-multiflow: A multi-output streaming framework. <em>Journal of Machine Learning Research</em>, 19(72):1–5, 2018.</li>
  <li>Jacob Montiel, Max Halford, Saulo Martiello Mastelini, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, Heitor Murilo Gomes, Jesse Read, Talel Abdessalem, and Albert Bifet. River: machine learning for streaming data in python. <em>Journal of Machine Learning Research</em>, 22:1–8, April 2021.</li>
  <li>Jacob Montiel, Hoang-Anh Ngo, Minh-Huong Le-Nguyen, and Albert Bifet. Online clustering: Algorithms, evaluation, metrics, applications and benchmarking. In <em>Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, KDD ’22, page 4808–4809, New York, NY, USA, 2022. Association for Computing Machinery.</li>
  <li>Jordan Pearson and Yoshua Bengio. When AI Goes Wrong, We Won’t Be Able to Ask It Why, July 2016.</li>
  <li>Anand Rajaraman and Jeffrey David Ullman. <em>Data Mining</em>, page 1–17. Cambridge University Press, 2011.</li>
  <li>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier, August 2016.</li>
  <li>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating errors. <em>Nature</em>, 323(6088):533–536, October 1986.</li>
  <li>Doyen Sahoo, Quang Pham, Jing Lu, and Steven C. H. Hoi. Online Deep Learning: Learning Deep Neural Networks on the Fly. In <em>IJCAI Proceedings</em>, pages 2660–2666, Stockholm, Sweden, July 2018. International Joint Conferences on Artificial Intelligence Organization.</li>
  <li>Sheng Wan and L.E. Banta. Parameter Incremental Learning Algorithm for Neural Networks. IEEE <em>Transactions on Neural Networks</em>, 17(6):1424–1438, November 2006.</li>
  <li>Jonathan A. Silva, Elaine R. Faria, Rodrigo C. Barros, Eduardo R. Hruschka, Andr ́e C. P. L. F. de Carvalho, and João Gama. Data stream clustering: A survey. <em>ACM Comput. Surv.</em>, 46(1), jul 2013.</li>
  <li>David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. <em>Nature</em>, 529(7587):484–489, January 2016.</li>
  <li>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks, February 2014.</li>
  <li>Vladimir Vapnik. <em>The Nature of Statistical Learning Theory</em>. Springer Science &amp; Business Media, November 1999.</li>
  <li>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. <em>Journal of Machine Learning Research</em>, page 38, 2010.</li>
  <li>Guanyu Zhou, Kihyuk Sohn, and Honglak Lee. Online Incremental Feature Learning with Denoising Autoencoders. In <em>AISTATS Proceedings</em>, 2012.</li>
</ol>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/hoanganhngo610/stream-clustering.ijcai.2023">This tutorial's website</a> is maintained by <a href="https://github.com/hoanganhngo610">Hoang Anh Ngo</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
